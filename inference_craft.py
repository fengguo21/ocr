"""
CRAFTæ–‡æœ¬æ£€æµ‹æ¨ç†è„šæœ¬
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡è¿›è¡Œæ–‡æœ¬æ£€æµ‹
"""

import torch
import cv2
import numpy as np
from PIL import Image
import argparse
import os
import matplotlib.pyplot as plt

from models.text_detection import CRAFT, TextDetector


class TrainedCRAFTDetector:
    """ä½¿ç”¨è®­ç»ƒå¥½çš„CRAFTæ¨¡å‹è¿›è¡Œæ–‡æœ¬æ£€æµ‹"""
    
    def __init__(self, model_path, device='cpu'):
        self.device = device
        
        # åˆ›å»ºæ¨¡å‹
        self.model = CRAFT(pretrained=False, freeze=False).to(device)
        
        # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
        if model_path and os.path.exists(model_path):
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
            checkpoint = torch.load(model_path, map_location=device)
            
            # å¤„ç†ä¸åŒçš„ä¿å­˜æ ¼å¼
            if 'model_state_dict' in checkpoint:
                # å®Œæ•´çš„checkpointæ ¼å¼
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print(f"æ¨¡å‹è®­ç»ƒepoch: {checkpoint.get('epoch', 'unknown')}")
                print(f"è®­ç»ƒæŸå¤±: {checkpoint.get('train_loss', 'unknown')}")
            else:
                # åªæœ‰æ¨¡å‹æƒé‡çš„æ ¼å¼
                self.model.load_state_dict(checkpoint)
            
            print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        else:
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            print("ä½¿ç”¨é¢„è®­ç»ƒæƒé‡æˆ–éšæœºåˆå§‹åŒ–çš„æƒé‡")
        
        self.model.eval()
    
    def detect_text(self, image_path, text_threshold=0.7, link_threshold=0.4, 
                   low_text=0.4, output_dir='results'):
        """
        æ£€æµ‹å›¾åƒä¸­çš„æ–‡æœ¬
        
        Args:
            image_path: è¾“å…¥å›¾åƒè·¯å¾„
            text_threshold: æ–‡æœ¬æ£€æµ‹é˜ˆå€¼
            link_threshold: é“¾æ¥æ£€æµ‹é˜ˆå€¼
            low_text: ä½æ–‡æœ¬é˜ˆå€¼
            output_dir: è¾“å‡ºç›®å½•
        """
        # è¯»å–å›¾åƒ
        if isinstance(image_path, str):
            image = cv2.imread(image_path)
            if image is None:
                raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            image = image_path
        
        # å›¾åƒé¢„å¤„ç†
        img_resized, target_ratio, size_heatmap = self._resize_aspect_ratio(
            image, 1280, interpolation=cv2.INTER_LINEAR, mag_ratio=1.5)
        ratio_h = ratio_w = 1 / target_ratio

        # è½¬æ¢ä¸ºtensor
        x = self._normalize_mean_variance(img_resized)
        x = torch.from_numpy(x).permute(2, 0, 1).unsqueeze(0).float().to(self.device)

        # æ¨ç†
        with torch.no_grad():
            y, _ = self.model(x)

        # åå¤„ç†
        score_text = y[0, :, :, 0].cpu().numpy()
        score_link = y[0, :, :, 1].cpu().numpy()

        # è·å–æ–‡æœ¬æ¡†
        boxes, polys = self._get_det_boxes(
            score_text, score_link, text_threshold, link_threshold, low_text)

        # è°ƒæ•´åæ ‡
        boxes = self._adjust_result_coordinates(boxes, ratio_w, ratio_h)
        polys = self._adjust_result_coordinates(polys, ratio_w, ratio_h)

        # ä¿å­˜ç»“æœ
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            self._save_results(image, boxes, score_text, score_link, 
                             output_dir, os.path.basename(image_path) if isinstance(image_path, str) else 'result')

        return boxes, polys, score_text, score_link
    
    def _resize_aspect_ratio(self, img, square_size, interpolation, mag_ratio=1):
        """è°ƒæ•´å›¾åƒå°ºå¯¸ä¿æŒå®½é«˜æ¯”"""
        height, width, channel = img.shape

        target_size = mag_ratio * max(height, width)
        if target_size > square_size:
            target_size = square_size
        
        ratio = target_size / max(height, width)    
        target_h, target_w = int(height * ratio), int(width * ratio)
        proc = cv2.resize(img, (target_w, target_h), interpolation=interpolation)

        # å¡«å……åˆ°32çš„å€æ•°
        target_h32, target_w32 = target_h, target_w
        if target_h % 32 != 0:
            target_h32 = target_h + (32 - target_h % 32)
        if target_w % 32 != 0:
            target_w32 = target_w + (32 - target_w % 32)
        resized = np.zeros((target_h32, target_w32, channel), dtype=np.uint8)
        resized[0:target_h, 0:target_w, :] = proc
        target_h, target_w = target_h32, target_w32

        size_heatmap = (int(target_w/2), int(target_h/2))
        return resized, ratio, size_heatmap

    def _normalize_mean_variance(self, in_img, mean=(0.485, 0.456, 0.406), 
                              variance=(0.229, 0.224, 0.225)):
        """å›¾åƒå½’ä¸€åŒ–"""
        img = in_img.copy().astype(np.float32)
        img -= np.array([mean[0] * 255.0, mean[1] * 255.0, mean[2] * 255.0], dtype=np.float32)
        img /= np.array([variance[0] * 255.0, variance[1] * 255.0, variance[2] * 255.0], dtype=np.float32)
        return img

    def _get_det_boxes(self, textmap, linkmap, text_threshold, link_threshold, low_text):
        """ä»çƒ­åŠ›å›¾æå–æ–‡æœ¬æ¡†"""
        img_h, img_w = textmap.shape

        ret, text_score = cv2.threshold(textmap, low_text, 1, cv2.THRESH_BINARY)
        ret, link_score = cv2.threshold(linkmap, link_threshold, 1, cv2.THRESH_BINARY)

        text_score_comb = np.clip(text_score + link_score, 0, 1)
        nLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(
            text_score_comb.astype(np.uint8), connectivity=4)

        det = []
        for k in range(1, nLabels):
            # è¿‡æ»¤å°åŒºåŸŸ
            size = stats[k, cv2.CC_STAT_AREA]
            if size < 10:
                continue

            # æ£€æŸ¥æ–‡æœ¬å¾—åˆ†
            if np.max(textmap[labels == k]) < text_threshold:
                continue

            # è·å–è¾¹ç•Œæ¡†
            segmap = np.zeros(textmap.shape, dtype=np.uint8)
            segmap[labels == k] = 255
            segmap[np.logical_and(link_score == 1, text_score == 0)] = 0

            x, y, w, h = cv2.boundingRect(segmap)
            niter = int(min(w, h) * 0.03)
            sx, ex, sy, ey = x - niter, x + w + niter + 1, y - niter, y + h + niter + 1
            
            # è¾¹ç•Œæ£€æŸ¥
            sx, sy = max(0, sx), max(0, sy)
            ex, ey = min(img_w, ex), min(img_h, ey)
            
            kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(1 + niter, 1 + niter))
            segmap[sy:ey, sx:ex] = cv2.dilate(segmap[sy:ey, sx:ex], kernel)

            # è·å–è½®å»“
            contour_info = cv2.findContours(segmap, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
            contours = contour_info[1] if len(contour_info) == 3 else contour_info[0]
                
            if len(contours) == 0:
                continue
            
            contour = contours[0]
            rect = cv2.minAreaRect(contour)
            points = cv2.boxPoints(rect)
            det.append(points)

        return det, det

    def _adjust_result_coordinates(self, polys, ratio_w, ratio_h):
        """è°ƒæ•´åæ ‡åˆ°åŸå›¾å°ºå¯¸"""
        if len(polys) > 0:
            polys = np.array(polys)
            for k in range(len(polys)):
                if polys[k] is not None:
                    polys[k] *= (ratio_w * 2, ratio_h * 2)
        return polys

    def _save_results(self, image, boxes, score_text, score_link, output_dir, filename):
        """ä¿å­˜æ£€æµ‹ç»“æœ"""
        # ä¿å­˜å¸¦æ¡†çš„å›¾åƒ
        result_img = image.copy()
        for box in boxes:
            if box is not None:
                box = box.astype(np.int32)
                cv2.polylines(result_img, [box], True, (0, 255, 0), 2)
        
        # åˆ›å»ºå¯è§†åŒ–
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # åŸå›¾
        axes[0, 0].imshow(image)
        axes[0, 0].set_title('åŸå›¾')
        axes[0, 0].axis('off')
        
        # æ£€æµ‹ç»“æœ
        axes[0, 1].imshow(result_img)
        axes[0, 1].set_title(f'æ£€æµ‹ç»“æœ ({len(boxes)} ä¸ªæ–‡æœ¬åŒºåŸŸ)')
        axes[0, 1].axis('off')
        
        # å­—ç¬¦çƒ­å›¾
        axes[1, 0].imshow(score_text, cmap='hot')
        axes[1, 0].set_title('å­—ç¬¦çƒ­å›¾')
        axes[1, 0].axis('off')
        
        # é“¾æ¥çƒ­å›¾
        axes[1, 1].imshow(score_link, cmap='hot')
        axes[1, 1].set_title('é“¾æ¥çƒ­å›¾')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        output_path = os.path.join(output_dir, f'detection_{filename}.png')
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ£€æµ‹ç»“æœå·²ä¿å­˜: {output_path}")


def main():
    parser = argparse.ArgumentParser(description='CRAFTæ–‡æœ¬æ£€æµ‹æ¨ç†')
    parser.add_argument('--model_path', type=str, help='è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--image_path', type=str, required=True, help='è¾“å…¥å›¾åƒè·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='detection_results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--text_threshold', type=float, default=0.7, help='æ–‡æœ¬æ£€æµ‹é˜ˆå€¼')
    parser.add_argument('--link_threshold', type=float, default=0.4, help='é“¾æ¥æ£€æµ‹é˜ˆå€¼')
    parser.add_argument('--device', type=str, default='cpu', help='è®¾å¤‡ (cpu/cuda)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è®¾å¤‡
    device = args.device
    if device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        device = 'cpu'
    
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = TrainedCRAFTDetector(args.model_path, device)
    
    # è¿›è¡Œæ£€æµ‹
    print(f"æ­£åœ¨æ£€æµ‹å›¾åƒ: {args.image_path}")
    boxes, polys, score_text, score_link = detector.detect_text(
        args.image_path,
        text_threshold=args.text_threshold,
        link_threshold=args.link_threshold,
        output_dir=args.output_dir
    )
    
    print(f"ğŸ¯ æ£€æµ‹å®Œæˆï¼æ‰¾åˆ° {len(boxes)} ä¸ªæ–‡æœ¬åŒºåŸŸ")


if __name__ == "__main__":
    main() 