"""
ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®æ ¼å¼è®­ç»ƒCRAFTæ¨¡å‹
é€‚é…boxã€clsã€wordæ ¼å¼çš„æ•°æ®
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import os
from tqdm import tqdm
import argparse

from models.text_detection import CRAFT
from loaddata import OCRDataset
from datasets import load_dataset


class CRAFTLoss(nn.Module):
    """CRAFTæŸå¤±å‡½æ•°"""
    
    def __init__(self):
        super(CRAFTLoss, self).__init__()
        self.mse_loss = nn.MSELoss()
    
    def forward(self, y_true_cls, y_pred_cls, y_true_geo, y_pred_geo):
        # å­—ç¬¦æ£€æµ‹æŸå¤±
        cls_loss = self.mse_loss(y_pred_cls, y_true_cls)
        
        # é“¾æ¥æ£€æµ‹æŸå¤±
        geo_loss = self.mse_loss(y_pred_geo, y_true_geo)
        
        # æ€»æŸå¤±
        total_loss = cls_loss + geo_loss
        
        return total_loss, cls_loss, geo_loss


def train_epoch(model, dataloader, optimizer, criterion, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    total_cls_loss = 0
    total_geo_loss = 0
    
    pbar = tqdm(dataloader, desc='è®­ç»ƒä¸­')
    
    for batch_idx, (images, char_heatmaps, link_heatmaps) in enumerate(pbar):
        images = images.to(device)
        char_heatmaps = char_heatmaps.to(device)
        link_heatmaps = link_heatmaps.to(device)
        
        optimizer.zero_grad()
        
        try:
            # å‰å‘ä¼ æ’­
            outputs, _ = model(images)
            
            # æå–é¢„æµ‹ç»“æœ
            pred_char = outputs[:, :, :, 0]  # å­—ç¬¦çƒ­å›¾
            pred_link = outputs[:, :, :, 1]  # é“¾æ¥çƒ­å›¾
            
            # è®¡ç®—æŸå¤±
            loss, cls_loss, geo_loss = criterion(
                char_heatmaps, pred_char,
                link_heatmaps, pred_link
            )
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
            
            optimizer.step()
            
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()
            total_geo_loss += geo_loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'cls': f'{cls_loss.item():.4f}',
                'geo': f'{geo_loss.item():.4f}'
            })
            
        except Exception as e:
            print(f"è®­ç»ƒæ‰¹æ¬¡å‡ºé”™: {e}")
            continue
    
    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0
    avg_cls_loss = total_cls_loss / len(dataloader) if len(dataloader) > 0 else 0
    avg_geo_loss = total_geo_loss / len(dataloader) if len(dataloader) > 0 else 0
    
    return avg_loss, avg_cls_loss, avg_geo_loss


def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒCRAFTæ¨¡å‹ï¼ˆè‡ªå®šä¹‰æ•°æ®æ ¼å¼ï¼‰')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹å¤§å°')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--save_dir', type=str, default='checkpoints_custom', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--resume', type=str, help='æ¢å¤è®­ç»ƒçš„æ¨¡å‹è·¯å¾„')
    
    args = parser.parse_args()
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®é›†
    print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    ds = load_dataset("lansinuote/ocr_id_card")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = OCRDataset(ds, split='train')
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(train_dataset))
    val_size = len(train_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        train_dataset, [train_size, val_size]
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0
    )
    
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset)}, éªŒè¯æ ·æœ¬: {len(val_dataset)}")
    
    # åˆ›å»ºæ¨¡å‹
    model = CRAFT(pretrained=True, freeze=False).to(device)
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=args.lr)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
    criterion = CRAFTLoss()
    
    # æ¢å¤è®­ç»ƒ
    start_epoch = 0
    if args.resume and os.path.exists(args.resume):
        checkpoint = torch.load(args.resume, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        print(f"ä»epoch {start_epoch}æ¢å¤è®­ç»ƒ")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    
    for epoch in range(start_epoch, args.epochs):
        print(f"\n=== Epoch {epoch+1}/{args.epochs} ===")
        
        # è®­ç»ƒ
        train_loss, train_cls_loss, train_geo_loss = train_epoch(
            model, train_loader, optimizer, criterion, device
        )
        
        print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f} (å­—ç¬¦: {train_cls_loss:.4f}, é“¾æ¥: {train_geo_loss:.4f})")
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if train_loss < best_loss:
            best_loss = train_loss
            
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'train_cls_loss': train_cls_loss,
                'train_geo_loss': train_geo_loss
            }
            
            torch.save(checkpoint, os.path.join(args.save_dir, 'best_model.pth'))
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒæŸå¤±: {best_loss:.4f}")
        
        # å®šæœŸä¿å­˜
        if epoch % 1 == 0:
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'train_cls_loss': train_cls_loss,
                'train_geo_loss': train_geo_loss
            }
            
            torch.save(checkpoint, os.path.join(args.save_dir, f'checkpoint_epoch_{epoch}.pth'))
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: epoch {epoch}")
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main() 