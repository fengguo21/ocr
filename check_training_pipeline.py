"""
å®Œæ•´æ£€æŸ¥è®­ç»ƒæµç¨‹
ç¡®ä¿æ‰€æœ‰ç¯èŠ‚éƒ½æ­£ç¡®æ— è¯¯
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
from datasets import load_dataset

from models.text_detection import CRAFT
from loaddata import OCRDataset
from train_with_your_data import CRAFTLoss


def check_data_pipeline():
    """æ£€æŸ¥æ•°æ®ç®¡é“"""
    print("ğŸ” æ£€æŸ¥æ•°æ®ç®¡é“...")
    
    # åŠ è½½æ•°æ®é›†
    ds = load_dataset("lansinuote/ocr_id_card")
    train_dataset = OCRDataset(ds, split='train')
    
    # æ•°æ®é›†åŸºæœ¬ä¿¡æ¯
    print(f"âœ… æ•°æ®é›†å¤§å°: {len(train_dataset)}")
    
    # æ£€æŸ¥æ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨æ‰¹æ¬¡æ•°: {len(dataloader)}")
    
    # æ£€æŸ¥å‡ ä¸ªæ‰¹æ¬¡
    batch_count = 0
    error_count = 0
    
    for batch_idx, (images, char_heatmaps, link_heatmaps) in enumerate(dataloader):
        try:
            # æ£€æŸ¥æ•°æ®å½¢çŠ¶
            assert images.shape[0] <= 4, f"æ‰¹å¤§å°é”™è¯¯: {images.shape[0]}"
            assert images.shape[1:] == (3, 512, 512), f"å›¾åƒå½¢çŠ¶é”™è¯¯: {images.shape[1:]}"
            assert char_heatmaps.shape[1:] == (256, 256), f"å­—ç¬¦çƒ­å›¾å½¢çŠ¶é”™è¯¯: {char_heatmaps.shape[1:]}"
            assert link_heatmaps.shape[1:] == (256, 256), f"é“¾æ¥çƒ­å›¾å½¢çŠ¶é”™è¯¯: {link_heatmaps.shape[1:]}"
            
            # æ£€æŸ¥æ•°æ®ç±»å‹
            assert images.dtype == torch.float32, f"å›¾åƒæ•°æ®ç±»å‹é”™è¯¯: {images.dtype}"
            assert char_heatmaps.dtype == torch.float32, f"å­—ç¬¦çƒ­å›¾æ•°æ®ç±»å‹é”™è¯¯: {char_heatmaps.dtype}"
            assert link_heatmaps.dtype == torch.float32, f"é“¾æ¥çƒ­å›¾æ•°æ®ç±»å‹é”™è¯¯: {link_heatmaps.dtype}"
            
            # æ£€æŸ¥æ•°å€¼èŒƒå›´
            assert 0 <= images.min() and images.max() <= 1, f"å›¾åƒå€¼åŸŸé”™è¯¯: [{images.min():.3f}, {images.max():.3f}]"
            assert 0 <= char_heatmaps.min() and char_heatmaps.max() <= 1, f"å­—ç¬¦çƒ­å›¾å€¼åŸŸé”™è¯¯: [{char_heatmaps.min():.3f}, {char_heatmaps.max():.3f}]"
            assert 0 <= link_heatmaps.min() and link_heatmaps.max() <= 1, f"é“¾æ¥çƒ­å›¾å€¼åŸŸé”™è¯¯: [{link_heatmaps.min():.3f}, {link_heatmaps.max():.3f}]"
            
            batch_count += 1
            
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ {batch_idx} æ•°æ®é”™è¯¯: {e}")
            error_count += 1
        
        if batch_idx >= 10:  # åªæ£€æŸ¥å‰10ä¸ªæ‰¹æ¬¡
            break
    
    print(f"âœ… æ•°æ®æ£€æŸ¥å®Œæˆ: {batch_count}ä¸ªæ­£å¸¸æ‰¹æ¬¡, {error_count}ä¸ªé”™è¯¯æ‰¹æ¬¡")
    return error_count == 0


def check_model_pipeline():
    """æ£€æŸ¥æ¨¡å‹ç®¡é“"""
    print("\nğŸ” æ£€æŸ¥æ¨¡å‹ç®¡é“...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    model = CRAFT(pretrained=True, freeze=False).to(device)
    
    # æ£€æŸ¥æ¨¡å‹å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"âœ… æ€»å‚æ•°: {total_params:,}")
    print(f"âœ… å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # æ£€æŸ¥æ¨¡å‹è¾“å‡º
    test_input = torch.randn(2, 3, 512, 512).to(device)
    
    try:
        model.eval()
        with torch.no_grad():
            outputs, features = model(test_input)
        
        print(f"âœ… æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
        print(f"âœ… ç‰¹å¾å›¾å½¢çŠ¶: {features.shape}")
        
        # æ£€æŸ¥è¾“å‡ºå€¼åŸŸ
        assert outputs.shape == (2, 256, 256, 2), f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {outputs.shape}"
        
        pred_char = outputs[:, :, :, 0]
        pred_link = outputs[:, :, :, 1]
        
        char_in_range = (pred_char >= 0).all() and (pred_char <= 1).all()
        link_in_range = (pred_link >= 0).all() and (pred_link <= 1).all()
        
        print(f"âœ… å­—ç¬¦è¾“å‡ºå€¼åŸŸ[0,1]: {'æ˜¯' if char_in_range else 'å¦'}")
        print(f"âœ… é“¾æ¥è¾“å‡ºå€¼åŸŸ[0,1]: {'æ˜¯' if link_in_range else 'å¦'}")
        
        if not char_in_range or not link_in_range:
            print(f"âŒ è¾“å‡ºå€¼åŸŸé”™è¯¯! å­—ç¬¦: [{pred_char.min():.3f}, {pred_char.max():.3f}], é“¾æ¥: [{pred_link.min():.3f}, {pred_link.max():.3f}]")
            return False
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹å‰å‘ä¼ æ’­é”™è¯¯: {e}")
        return False
    
    return True


def check_loss_pipeline():
    """æ£€æŸ¥æŸå¤±è®¡ç®—ç®¡é“"""
    print("\nğŸ” æ£€æŸ¥æŸå¤±è®¡ç®—ç®¡é“...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    criterion = CRAFTLoss()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ® - ğŸ”§ ä¿®å¤ï¼šé¢„æµ‹å¼ é‡éœ€è¦æ¢¯åº¦
    batch_size = 2
    target_char = torch.rand(batch_size, 256, 256).to(device)
    target_link = torch.rand(batch_size, 256, 256).to(device)
    pred_char = torch.rand(batch_size, 256, 256, requires_grad=True).to(device)
    pred_link = torch.rand(batch_size, 256, 256, requires_grad=True).to(device)
    
    try:
        loss, cls_loss, geo_loss = criterion(target_char, pred_char, target_link, pred_link)
        
        print(f"âœ… æŸå¤±è®¡ç®—æˆåŠŸ")
        print(f"  æ€»æŸå¤±: {loss.item():.6f}")
        print(f"  å­—ç¬¦æŸå¤±: {cls_loss.item():.6f}")
        print(f"  é“¾æ¥æŸå¤±: {geo_loss.item():.6f}")
        
        # æ£€æŸ¥æŸå¤±å€¼åˆç†æ€§
        if loss.item() > 2.0:
            print(f"âš ï¸ æŸå¤±å€¼è¾ƒé«˜: {loss.item():.6f}")
        elif loss.item() < 0.0:
            print(f"âŒ æŸå¤±å€¼ä¸ºè´Ÿ: {loss.item():.6f}")
            return False
        else:
            print(f"âœ… æŸå¤±å€¼åˆç†")
        
        # æ£€æŸ¥æ¢¯åº¦è®¡ç®—
        loss.backward()
        print(f"âœ… åå‘ä¼ æ’­æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æŸå¤±è®¡ç®—é”™è¯¯: {e}")
        return False
    
    return True


def check_training_integration():
    """æ£€æŸ¥è®­ç»ƒé›†æˆæµ‹è¯•"""
    print("\nğŸ” æ£€æŸ¥è®­ç»ƒé›†æˆ...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åŠ è½½æ•°æ®
    ds = load_dataset("lansinuote/ocr_id_card")
    train_dataset = OCRDataset(ds, split='train')
    dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)
    
    # åˆ›å»ºæ¨¡å‹
    model = CRAFT(pretrained=True, freeze=False).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
    criterion = CRAFTLoss()
    
    # æ¨¡æ‹Ÿä¸€ä¸ªè®­ç»ƒæ­¥éª¤
    model.train()
    
    try:
        images, char_heatmaps, link_heatmaps = next(iter(dataloader))
        images = images.to(device)
        char_heatmaps = char_heatmaps.to(device)
        link_heatmaps = link_heatmaps.to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        outputs, _ = model(images)
        
        pred_char = outputs[:, :, :, 0]
        pred_link = outputs[:, :, :, 1]
        
        # æŸå¤±è®¡ç®—
        loss, cls_loss, geo_loss = criterion(char_heatmaps, pred_char, link_heatmaps, pred_link)
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦æ£€æŸ¥
        grad_norms = []
        for param in model.parameters():
            if param.grad is not None:
                grad_norms.append(param.grad.data.norm(2).item())
        
        max_grad = max(grad_norms) if grad_norms else 0
        avg_grad = np.mean(grad_norms) if grad_norms else 0
        
        print(f"âœ… å®Œæ•´è®­ç»ƒæ­¥éª¤æˆåŠŸ")
        print(f"  æŸå¤±: {loss.item():.6f}")
        print(f"  æœ€å¤§æ¢¯åº¦: {max_grad:.6f}")
        print(f"  å¹³å‡æ¢¯åº¦: {avg_grad:.6f}")
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        
        # å‚æ•°æ›´æ–°
        optimizer.step()
        
        print(f"âœ… å‚æ•°æ›´æ–°æˆåŠŸ")
        
        # æ£€æŸ¥æ¢¯åº¦çˆ†ç‚¸
        if max_grad > 10.0:
            print(f"âš ï¸ æ¢¯åº¦è¿‡å¤§: {max_grad:.6f}")
            return False
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒé›†æˆé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


def check_training_script_issues():
    """æ£€æŸ¥è®­ç»ƒè„šæœ¬ä¸­çš„æ½œåœ¨é—®é¢˜"""
    print("\nğŸ” æ£€æŸ¥è®­ç»ƒè„šæœ¬é—®é¢˜...")
    
    issues = []
    
    # 1. éªŒè¯å¾ªç¯ç¼ºå¤±
    print("ğŸ“‹ è®­ç»ƒè„šæœ¬åˆ†æ:")
    print("  âš ï¸ é—®é¢˜1: ç¼ºå°‘éªŒè¯å¾ªç¯")
    print("    - æœ‰éªŒè¯æ•°æ®é›†åˆ†å‰²ï¼Œä½†æœªåœ¨è®­ç»ƒä¸­ä½¿ç”¨")
    print("    - æ— æ³•ç›‘æ§è¿‡æ‹Ÿåˆ")
    issues.append("validation_loop")
    
    # 2. ä¿å­˜ç­–ç•¥
    print("  âš ï¸ é—®é¢˜2: ä¿å­˜ç­–ç•¥å¯èƒ½å ç”¨å¤§é‡ç©ºé—´")
    print("    - æ¯ä¸ªepochéƒ½ä¿å­˜æ£€æŸ¥ç‚¹")
    print("    - å»ºè®®åªä¿å­˜æœ€ä½³æ¨¡å‹å’Œå®šæœŸæ£€æŸ¥ç‚¹")
    issues.append("save_strategy")
    
    # 3. æ—©åœæœºåˆ¶
    print("  âš ï¸ é—®é¢˜3: ç¼ºå°‘æ—©åœæœºåˆ¶")
    print("    - æ²¡æœ‰åŸºäºéªŒè¯æŸå¤±çš„æ—©åœ")
    print("    - å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆ")
    issues.append("early_stopping")
    
    # 4. å­¦ä¹ ç‡è°ƒåº¦
    print("  âœ… å­¦ä¹ ç‡è°ƒåº¦: StepLRæ­£å¸¸")
    
    # 5. æ¢¯åº¦è£å‰ª
    print("  âœ… æ¢¯åº¦è£å‰ª: å·²è®¾ç½®ä¸º1.0")
    
    # 6. å¼‚å¸¸å¤„ç†
    print("  âœ… å¼‚å¸¸å¤„ç†: æœ‰å¼‚å¸¸æ•è·æœºåˆ¶")
    
    return issues


def main():
    """ä¸»æ£€æŸ¥å‡½æ•°"""
    print("ğŸ” å¼€å§‹å®Œæ•´è®­ç»ƒæµç¨‹æ£€æŸ¥...\n")
    
    # æ£€æŸ¥å„ä¸ªç»„ä»¶
    data_ok = check_data_pipeline()
    model_ok = check_model_pipeline()
    loss_ok = check_loss_pipeline()
    training_ok = check_training_integration()
    
    # æ£€æŸ¥è„šæœ¬é—®é¢˜
    script_issues = check_training_script_issues()
    
    print(f"\nğŸ“Š æ£€æŸ¥ç»“æœæ€»ç»“:")
    print(f"  æ•°æ®ç®¡é“: {'âœ… æ­£å¸¸' if data_ok else 'âŒ å¼‚å¸¸'}")
    print(f"  æ¨¡å‹ç®¡é“: {'âœ… æ­£å¸¸' if model_ok else 'âŒ å¼‚å¸¸'}")
    print(f"  æŸå¤±ç®¡é“: {'âœ… æ­£å¸¸' if loss_ok else 'âŒ å¼‚å¸¸'}")
    print(f"  è®­ç»ƒé›†æˆ: {'âœ… æ­£å¸¸' if training_ok else 'âŒ å¼‚å¸¸'}")
    print(f"  è„šæœ¬é—®é¢˜: {len(script_issues)}ä¸ª")
    
    all_ok = data_ok and model_ok and loss_ok and training_ok
    
    if all_ok:
        print(f"\nğŸ‰ è®­ç»ƒæµç¨‹åŸºæœ¬æ­£å¸¸ï¼")
        if script_issues:
            print(f"ğŸ’¡ å»ºè®®æ”¹è¿›ä»¥ä¸‹é—®é¢˜:")
            for issue in script_issues:
                if issue == "validation_loop":
                    print(f"  - æ·»åŠ éªŒè¯å¾ªç¯")
                elif issue == "save_strategy":
                    print(f"  - ä¼˜åŒ–ä¿å­˜ç­–ç•¥")
                elif issue == "early_stopping":
                    print(f"  - æ·»åŠ æ—©åœæœºåˆ¶")
    else:
        print(f"\nâŒ è®­ç»ƒæµç¨‹å­˜åœ¨é—®é¢˜ï¼Œéœ€è¦ä¿®å¤ï¼")
    
    return all_ok, script_issues


if __name__ == "__main__":
    success, issues = main()
    
    if success:
        print(f"\nâœ… å¯ä»¥å¼€å§‹è®­ç»ƒï¼Œä½†å»ºè®®ä¼˜åŒ–è„šæœ¬é—®é¢˜")
    else:
        print(f"\nâŒ è¯·å…ˆä¿®å¤é”™è¯¯å†å¼€å§‹è®­ç»ƒ") 