import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2
import numpy as np
import os
from torchvision import models


class DoubleConv(nn.Module):
    """åŒé‡å·ç§¯å—"""
    def __init__(self, in_ch, mid_ch, out_ch):
        super(DoubleConv, self).__init__()
        # å¦‚æœin_chä¸º0ï¼Œè¡¨ç¤ºä¸éœ€è¦é¢å¤–è¾“å…¥é€šé“
        print(in_ch,mid_ch,out_ch,"in_ch,mid_ch,out_ch")
        if in_ch == 0:
            self.conv = nn.Sequential(
                nn.Conv2d(mid_ch, mid_ch, kernel_size=1),
                nn.BatchNorm2d(mid_ch),
                nn.ReLU(inplace=True),
                nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1),
                nn.BatchNorm2d(out_ch),
                nn.ReLU(inplace=True)
            )
        else:
            self.conv = nn.Sequential(
                nn.Conv2d(in_ch + mid_ch, mid_ch, kernel_size=1),
                nn.BatchNorm2d(mid_ch),
                nn.ReLU(inplace=True),
                nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1),
                nn.BatchNorm2d(out_ch),
                nn.ReLU(inplace=True)
            )

    def forward(self, x):
        x = self.conv(x)
        return x


class CRAFT(nn.Module):
    """CRAFTæ–‡æœ¬æ£€æµ‹æ¨¡å‹"""
    
    def __init__(self, pretrained=False, freeze=False):
        super(CRAFT, self).__init__()

        # ä½¿ç”¨VGG16ä½œä¸ºéª¨å¹²ç½‘ç»œ
        vgg16 = models.vgg16(pretrained=pretrained)
        self.basenet = nn.ModuleList(list(vgg16.features.children()))

        # ä¸Šé‡‡æ ·ç½‘ç»œ - ä¿®å¤é€šé“æ•°é…ç½®
        self.upconv1 = DoubleConv(0, 512, 256)    # conv5_3: 512 channels -> 256
        self.upconv2 = DoubleConv(512, 256, 128)  # upconv1: 256 + conv4_3: 512 = 768 -> 128
        self.upconv3 = DoubleConv(128, 256, 64)   # upconv2: 128 + conv3_3: 256 = 384 -> 64
        self.upconv4 = DoubleConv(0, 64, 32)      # upconv3: 64 -> 32

        num_class = 2
        self.conv_cls = nn.Sequential(
            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(32, 16, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(16, 16, kernel_size=1), nn.ReLU(inplace=True),
            nn.Conv2d(16, num_class, kernel_size=1),
            nn.Sigmoid()  # ğŸ”§ æ·»åŠ Sigmoidæ¿€æ´»å‡½æ•°ï¼Œç¡®ä¿è¾“å‡ºåœ¨[0,1]èŒƒå›´
        )

        self.init_weights()

        if freeze:
            for param in self.basenet.parameters():
                param.requires_grad = False

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x):
        # VGG16ç‰¹å¾æå–
        sources = []
        for k in range(len(self.basenet)):
            x = self.basenet[k](x)
            if k in [15, 22, 29]:  # conv3_3, conv4_3, conv5_3
                sources.append(x)
                # print(x.shape,"x")

        # ä¸Šé‡‡æ ·å’Œç‰¹å¾èåˆ - ä¿®å¤é€šé“æ•°åŒ¹é…
        # conv5_3: 512 channelsï¼Œä¸éœ€è¦æ‹¼æ¥
        y = self.upconv1(sources[2])  # ç›´æ¥ä½¿ç”¨conv5_3  sources[2] 512 -> 256

        # ä¸Šé‡‡æ ·åˆ°conv4_3çš„å°ºå¯¸å¹¶æ‹¼æ¥
        y = F.interpolate(y, size=sources[1].size()[2:], mode='bilinear', align_corners=False)
        y = torch.cat([y, sources[1]], dim=1)  # 256 + 512 = 768 -> ä½¿ç”¨DoubleConvè°ƒæ•´
        y = self.upconv2(y)

        # ä¸Šé‡‡æ ·åˆ°conv3_3çš„å°ºå¯¸å¹¶æ‹¼æ¥
        y = F.interpolate(y, size=sources[0].size()[2:], mode='bilinear', align_corners=False)
        y = torch.cat([y, sources[0]], dim=1)  # 128 + 256 = 384 -> ä½¿ç”¨DoubleConvè°ƒæ•´
        y = self.upconv3(y)

        # æœ€ç»ˆä¸Šé‡‡æ ·
        y = F.interpolate(y, size=(sources[0].size(2)*2, sources[0].size(3)*2), mode='bilinear', align_corners=False)

        feature = self.upconv4(y)

        y = self.conv_cls(feature)

        return y.permute(0, 2, 3, 1), feature


class TextDetector:
    """æ–‡æœ¬æ£€æµ‹å™¨å°è£…ç±»"""
    
    def __init__(self, model_path=None, device='cpu', use_pretrained=True):
        self.device = device
        self.net = CRAFT(pretrained=use_pretrained, freeze=False).to(device)
        
        if model_path and os.path.exists(model_path):
            print(f"æ­£åœ¨åŠ è½½è‡ªå®šä¹‰æ¨¡å‹æƒé‡: {model_path}")
            checkpoint = torch.load(model_path, map_location=device)
            
            # å¤„ç†ä¸åŒçš„ä¿å­˜æ ¼å¼
            if 'model_state_dict' in checkpoint:
                # å®Œæ•´çš„checkpointæ ¼å¼
                self.net.load_state_dict(checkpoint['model_state_dict'])
                print(f"âœ… åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ - Epoch: {checkpoint.get('epoch', 'unknown')}")
                print(f"è®­ç»ƒæŸå¤±: {checkpoint.get('train_loss', 'unknown'):.4f}" if checkpoint.get('train_loss') else "")
            else:
                # åªæœ‰æ¨¡å‹æƒé‡çš„æ ¼å¼
                self.net.load_state_dict(checkpoint)
                print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        elif model_path:
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            if use_pretrained:
                print("ä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡")
            else:
                print("ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
        
        self.net.eval()
        
    def detect_text(self, image, text_threshold=0.7, link_threshold=0.4, 
                   low_text=0.4, poly=False):
        """
        æ£€æµ‹å›¾åƒä¸­çš„æ–‡æœ¬åŒºåŸŸ
        """
        # å›¾åƒé¢„å¤„ç†
        img_resized, target_ratio, size_heatmap = self.resize_aspect_ratio(
            image, 1280, interpolation=cv2.INTER_LINEAR, mag_ratio=1.5)
        ratio_h = ratio_w = 1 / target_ratio

        # è½¬æ¢ä¸ºtensor
        x = self.normalize_mean_variance(img_resized)
        x = torch.from_numpy(x).permute(2, 0, 1).unsqueeze(0).float().to(self.device)

        # æ¨ç†
        with torch.no_grad():
            y, _ = self.net(x)

        # åå¤„ç†
        score_text = y[0, :, :, 0].cpu().numpy()
        score_link = y[0, :, :, 1].cpu().numpy()

        # è·å–æ–‡æœ¬æ¡†
        boxes, polys = self.get_det_boxes(
            score_text, score_link, text_threshold, link_threshold, low_text, poly)

        # è°ƒæ•´åæ ‡
        boxes = self.adjust_result_coordinates(boxes, ratio_w, ratio_h)
        polys = self.adjust_result_coordinates(polys, ratio_w, ratio_h)
        print(boxes,"boxes")
        print(polys,"polys")
        

        return boxes, polys

    def resize_aspect_ratio(self, img, square_size, interpolation, mag_ratio=1):
        height, width, channel = img.shape

        # è®¡ç®—ç›®æ ‡å°ºå¯¸
        target_size = mag_ratio * max(height, width)

        if target_size > square_size:
            target_size = square_size
        
        ratio = target_size / max(height, width)    

        target_h, target_w = int(height * ratio), int(width * ratio)
        proc = cv2.resize(img, (target_w, target_h), interpolation=interpolation)

        # å¡«å……åˆ°æ­£æ–¹å½¢
        target_h32, target_w32 = target_h, target_w
        if target_h % 32 != 0:
            target_h32 = target_h + (32 - target_h % 32)
        if target_w % 32 != 0:
            target_w32 = target_w + (32 - target_w % 32)
        resized = np.zeros((target_h32, target_w32, channel), dtype=np.uint8)
        resized[0:target_h, 0:target_w, :] = proc
        target_h, target_w = target_h32, target_w32

        size_heatmap = (int(target_w/2), int(target_h/2))

        return resized, ratio, size_heatmap

    def normalize_mean_variance(self, in_img, mean=(0.485, 0.456, 0.406), 
                              variance=(0.229, 0.224, 0.225)):
        # åº”è¯¥åœ¨0~1èŒƒå›´å†…
        img = in_img.copy().astype(np.float32)

        img -= np.array([mean[0] * 255.0, mean[1] * 255.0, mean[2] * 255.0], dtype=np.float32)
        img /= np.array([variance[0] * 255.0, variance[1] * 255.0, variance[2] * 255.0], dtype=np.float32)
        return img

    def get_det_boxes(self, textmap, linkmap, text_threshold, link_threshold, low_text, poly=False):
        # å®ç°æ–‡æœ¬æ¡†æ£€æµ‹ç®—æ³•
        img_h, img_w = textmap.shape

        ret, text_score = cv2.threshold(textmap, low_text, 1, cv2.THRESH_BINARY)
        ret, link_score = cv2.threshold(linkmap, link_threshold, 1, cv2.THRESH_BINARY)

        text_score_comb = np.clip(text_score + link_score, 0, 1)
        nLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(
            text_score_comb.astype(np.uint8), connectivity=4)

        det = []
        for k in range(1, nLabels):
            # è¿‡æ»¤å°åŒºåŸŸ
            size = stats[k, cv2.CC_STAT_AREA]
            if size < 10:
                continue

            # è·å–è¾¹ç•Œæ¡†
            if np.max(textmap[labels == k]) < text_threshold:
                continue

            # è·å–è¾¹ç•Œæ¡†åæ ‡
            segmap = np.zeros(textmap.shape, dtype=np.uint8)
            segmap[labels == k] = 255
            segmap[np.logical_and(link_score == 1, text_score == 0)] = 0

            x, y, w, h = cv2.boundingRect(segmap)
            niter = int(min(w, h) * 0.03)
            sx, ex, sy, ey = x - niter, x + w + niter + 1, y - niter, y + h + niter + 1
            
            if sx < 0:
                sx = 0
            if sy < 0:
                sy = 0
            if ex >= img_w:
                ex = img_w
            if ey >= img_h:
                ey = img_h
            
            kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(1 + niter, 1 + niter))
            segmap[sy:ey, sx:ex] = cv2.dilate(segmap[sy:ey, sx:ex], kernel)

            # è·å–è½®å»“ - å…¼å®¹ä¸åŒOpenCVç‰ˆæœ¬
            contour_info = cv2.findContours(segmap, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
            if len(contour_info) == 3:
                # OpenCV 3.x è¿”å› (image, contours, hierarchy)
                contours = contour_info[1]
            else:
                # OpenCV 4.x è¿”å› (contours, hierarchy)
                contours = contour_info[0]
                
            if len(contours) == 0:
                continue
            
            contour = contours[0]
            
            # ç”Ÿæˆè¾¹ç•Œæ¡†
            if poly:
                epsilon = 0.002 * cv2.arcLength(contour, True)
                points = cv2.approxPolyDP(contour, epsilon, True)
                points = np.array(points).reshape((-1, 2))
            else:
                rect = cv2.minAreaRect(contour)
                points = cv2.boxPoints(rect)

            det.append(points)

        return det, det

    def adjust_result_coordinates(self, polys, ratio_w, ratio_h):
        if len(polys) > 0:
            polys = np.array(polys)
            for k in range(len(polys)):
                if polys[k] is not None:
                    polys[k] *= (ratio_w * 2, ratio_h * 2)
        return polys 