"""
æ”¹è¿›ç‰ˆCRAFTè®­ç»ƒè„šæœ¬
- æ·»åŠ éªŒè¯å¾ªç¯
- æ—©åœæœºåˆ¶
- ä¼˜åŒ–ä¿å­˜ç­–ç•¥
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import os
from tqdm import tqdm
import argparse
import time
from collections import defaultdict

from models.text_detection import CRAFT
from loaddata import OCRDataset
from datasets import load_dataset


class CRAFTLoss(nn.Module):
    """CRAFTæŸå¤±å‡½æ•°"""
    
    def __init__(self):
        super(CRAFTLoss, self).__init__()
        self.mse_loss = nn.MSELoss()
    
    def forward(self, y_true_cls, y_pred_cls, y_true_geo, y_pred_geo):
        cls_loss = self.mse_loss(y_pred_cls, y_true_cls)
        geo_loss = self.mse_loss(y_pred_geo, y_true_geo)
        total_loss = cls_loss + geo_loss
        return total_loss, cls_loss, geo_loss


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    
    def __init__(self, patience=7, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
        
    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            return False
        else:
            self.counter += 1
            return self.counter >= self.patience


def train_epoch(model, dataloader, optimizer, criterion, device, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    total_cls_loss = 0
    total_geo_loss = 0
    batch_count = 0
    error_count = 0
    
    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1} - è®­ç»ƒ')
    
    for batch_idx, (images, char_heatmaps, link_heatmaps) in enumerate(pbar):
        try:
            images = images.to(device)
            char_heatmaps = char_heatmaps.to(device)
            link_heatmaps = link_heatmaps.to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs, _ = model(images)
            
            # æå–é¢„æµ‹ç»“æœ
            pred_char = outputs[:, :, :, 0]
            pred_link = outputs[:, :, :, 1]
            
            # è®¡ç®—æŸå¤±
            loss, cls_loss, geo_loss = criterion(
                char_heatmaps, pred_char,
                link_heatmaps, pred_link
            )
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()
            total_geo_loss += geo_loss.item()
            batch_count += 1
            
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'cls': f'{cls_loss.item():.4f}',
                'geo': f'{geo_loss.item():.4f}',
                'errors': error_count
            })
            
        except Exception as e:
            error_count += 1
            print(f"è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
            continue
    
    avg_loss = total_loss / batch_count if batch_count > 0 else 0
    avg_cls_loss = total_cls_loss / batch_count if batch_count > 0 else 0
    avg_geo_loss = total_geo_loss / batch_count if batch_count > 0 else 0
    
    return avg_loss, avg_cls_loss, avg_geo_loss, error_count


def validate_epoch(model, dataloader, criterion, device, epoch):
    """éªŒè¯ä¸€ä¸ªepoch"""
    model.eval()
    total_loss = 0
    total_cls_loss = 0
    total_geo_loss = 0
    batch_count = 0
    
    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1} - éªŒè¯')
    
    with torch.no_grad():
        for batch_idx, (images, char_heatmaps, link_heatmaps) in enumerate(pbar):
            try:
                images = images.to(device)
                char_heatmaps = char_heatmaps.to(device)
                link_heatmaps = link_heatmaps.to(device)
                
                outputs, _ = model(images)
                pred_char = outputs[:, :, :, 0]
                pred_link = outputs[:, :, :, 1]
                
                loss, cls_loss, geo_loss = criterion(
                    char_heatmaps, pred_char,
                    link_heatmaps, pred_link
                )
                
                total_loss += loss.item()
                total_cls_loss += cls_loss.item()
                total_geo_loss += geo_loss.item()
                batch_count += 1
                
                pbar.set_postfix({
                    'val_loss': f'{loss.item():.4f}',
                    'val_cls': f'{cls_loss.item():.4f}',
                    'val_geo': f'{geo_loss.item():.4f}'
                })
                
            except Exception as e:
                print(f"éªŒè¯æ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                continue
    
    avg_loss = total_loss / batch_count if batch_count > 0 else 0
    avg_cls_loss = total_cls_loss / batch_count if batch_count > 0 else 0
    avg_geo_loss = total_geo_loss / batch_count if batch_count > 0 else 0
    
    return avg_loss, avg_cls_loss, avg_geo_loss


def main():
    parser = argparse.ArgumentParser(description='æ”¹è¿›ç‰ˆCRAFTè®­ç»ƒè„šæœ¬')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹å¤§å°')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=0.0001, help='å­¦ä¹ ç‡')
    parser.add_argument('--save_dir', type=str, default='checkpoints_improved', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--resume', type=str, help='æ¢å¤è®­ç»ƒçš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--patience', type=int, default=7, help='æ—©åœè€å¿ƒå€¼')
    
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®é›†
    print("ğŸ“Š æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    ds = load_dataset("lansinuote/ocr_id_card")
    full_dataset = OCRDataset(ds, split='train')
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)
    
    print(f"ğŸ“ˆ è®­ç»ƒæ ·æœ¬: {len(train_dataset)}, éªŒè¯æ ·æœ¬: {len(val_dataset)}")
    
    # åˆ›å»ºæ¨¡å‹
    model = CRAFT(pretrained=True, freeze=False).to(device)
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
    criterion = CRAFTLoss()
    early_stopping = EarlyStopping(patience=args.patience)
    
    os.makedirs(args.save_dir, exist_ok=True)
    
    best_val_loss = float('inf')
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    for epoch in range(args.epochs):
        # è®­ç»ƒ
        train_loss, train_cls_loss, train_geo_loss, train_errors = train_epoch(
            model, train_loader, optimizer, criterion, device, epoch
        )
        
        # éªŒè¯
        val_loss, val_cls_loss, val_geo_loss = validate_epoch(
            model, val_loader, criterion, device, epoch
        )
        
        scheduler.step()
        
        print(f"\nEpoch {epoch+1}/{args.epochs}")
        print(f"è®­ç»ƒ - æ€»æŸå¤±: {train_loss:.4f}, å­—ç¬¦: {train_cls_loss:.4f}, é“¾æ¥: {train_geo_loss:.4f}")
        print(f"éªŒè¯ - æ€»æŸå¤±: {val_loss:.4f}, å­—ç¬¦: {val_cls_loss:.4f}, é“¾æ¥: {val_geo_loss:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss
            }
            torch.save(checkpoint, os.path.join(args.save_dir, 'best_model.pth'))
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {best_val_loss:.4f}")
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(val_loss):
            print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼éªŒè¯æŸå¤±å·²è¿ç»­{args.patience}ä¸ªepochæœªæ”¹å–„")
            break
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")


if __name__ == "__main__":
    main() 