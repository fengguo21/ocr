"""
CRNNæ¨¡å‹è®­ç»ƒè„šæœ¬
ç”¨äºè®­ç»ƒèº«ä»½è¯æ–‡æœ¬è¯†åˆ«æ¨¡å‹
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import cv2
import numpy as np
from PIL import Image
import json
import argparse
from tqdm import tqdm
import logging
from pathlib import Path

from models.crnn import CRNN
from utils.image_processing import IDCardPreprocessor


class TextDataset(Dataset):
    """æ–‡æœ¬è¯†åˆ«æ•°æ®é›†"""
    
    def __init__(self, data_dir, label_file, charset, transform=None):
        self.data_dir = Path(data_dir)
        self.transform = transform
        self.charset = charset
        self.char_to_idx = {char: idx for idx, char in enumerate(charset)}
        
        # åŠ è½½æ ‡ç­¾ - æ”¯æŒtxtæ ¼å¼
        self.samples = []
        if label_file.endswith('.txt'):
            with open(label_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line:
                        parts = line.split(' ', 1)
                        if len(parts) == 2:
                            image_name, text = parts
                            self.samples.append({
                                'image': image_name,
                                'text': text
                            })
        else:
            # JSONæ ¼å¼
            with open(label_file, 'r', encoding='utf-8') as f:
                self.samples = json.load(f)
        
        print(f"åŠ è½½ {len(self.samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        image_path = self.data_dir / sample['image']
        text = sample['text']
        
        # åŠ è½½å›¾åƒ
        image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)
        if image is None:
            raise ValueError(f"æ— æ³•åŠ è½½å›¾åƒ: {image_path}")
        
        # å›¾åƒé¢„å¤„ç†
        if self.transform:
            image = self.transform(image)
        
        # è°ƒæ•´å›¾åƒå°ºå¯¸
        image = self._resize_image(image)
        
        # è½¬æ¢ä¸ºtensor
        image = torch.from_numpy(image).float() / 255.0
        image = image.unsqueeze(0)  # æ·»åŠ é€šé“ç»´åº¦
        
        # ç¼–ç æ–‡æœ¬
        encoded_text = self._encode_text(text)
        
        return image, encoded_text, text
    
    def _resize_image(self, image, target_height=32):
        """è°ƒæ•´å›¾åƒå°ºå¯¸"""
        h, w = image.shape
        target_width = int(w * target_height / h)
        
        # ç¡®ä¿å®½åº¦åœ¨åˆç†èŒƒå›´å†…
        min_width = 16
        max_width = 160
        target_width = max(min_width, min(target_width, max_width))
        
        resized = cv2.resize(image, (target_width, target_height))
        return resized
    
    def _encode_text(self, text):
        """ç¼–ç æ–‡æœ¬ä¸ºæ•°å­—åºåˆ—"""
        encoded = []
        for char in text:
            if char in self.char_to_idx:
                encoded.append(self.char_to_idx[char])
            else:
                # æœªçŸ¥å­—ç¬¦ç”¨0è¡¨ç¤º
                print(f"è­¦å‘Š: æœªçŸ¥å­—ç¬¦ '{char}' åœ¨æ–‡æœ¬ '{text}' ä¸­")
                encoded.append(0)
        return encoded


def collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°"""
    images, targets, texts = zip(*batch)
    
    # è·å–æ‰¹æ¬¡ä¸­çš„æœ€å¤§å®½åº¦
    max_width = max(img.shape[2] for img in images)
    batch_size = len(images)
    
    # åˆ›å»ºå¡«å……åçš„å›¾åƒtensor
    padded_images = torch.zeros(batch_size, 1, 32, max_width)
    target_lengths = []
    all_targets = []
    
    for i, (img, target, text) in enumerate(zip(images, targets, texts)):
        # å¡«å……å›¾åƒ
        padded_images[i, :, :, :img.shape[2]] = img
        
        # æ”¶é›†ç›®æ ‡
        all_targets.extend(target)
        target_lengths.append(len(target))
    
    # è®¡ç®—è¾“å…¥é•¿åº¦ï¼ˆåŸºäºå›¾åƒå®½åº¦ï¼‰
    input_lengths = [max_width // 4 for _ in images]  # CNNä¸‹é‡‡æ ·4å€
    
    return (padded_images, 
            torch.tensor(all_targets, dtype=torch.long),
            torch.tensor(input_lengths, dtype=torch.long),
            torch.tensor(target_lengths, dtype=torch.long),
            texts)


class CTCTrainer:
    """CTCè®­ç»ƒå™¨"""
    
    def __init__(self, model, device, charset):
        self.model = model
        self.device = device
        self.charset = charset
        self.criterion = nn.CTCLoss(blank=len(charset), zero_infinity=True)
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def train_epoch(self, dataloader, optimizer, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        pbar = tqdm(dataloader, desc=f'Epoch {epoch}')
        
        for batch_idx, (images, targets, input_lengths, target_lengths, texts) in enumerate(pbar):
            images = images.to(self.device)
            targets = targets.to(self.device)
            input_lengths = input_lengths.to(self.device)
            target_lengths = target_lengths.to(self.device)
            
            optimizer.zero_grad()
            
            try:
                # å‰å‘ä¼ æ’­
                outputs = self.model(images)
                outputs = outputs.log_softmax(2)
                
                # è®¡ç®—CTCæŸå¤±
                loss = self.criterion(outputs, targets, input_lengths, target_lengths)
                
                # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"è·³è¿‡æ— æ•ˆæŸå¤±: {loss.item()}")
                    continue
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)
                
                optimizer.step()
                
                total_loss += loss.item()
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({'loss': loss.item()})
                
            except Exception as e:
                print(f"è®­ç»ƒæ‰¹æ¬¡å‡ºé”™: {e}")
                continue
        
        avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0
        self.logger.info(f'Epoch {epoch}, Average Loss: {avg_loss:.4f}')
        
        return avg_loss
    
    def validate(self, dataloader):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, targets, input_lengths, target_lengths, texts in dataloader:
                images = images.to(self.device)
                targets = targets.to(self.device)
                input_lengths = input_lengths.to(self.device)
                target_lengths = target_lengths.to(self.device)
                
                try:
                    # å‰å‘ä¼ æ’­
                    outputs = self.model(images)
                    outputs = outputs.log_softmax(2)
                    
                    # è®¡ç®—æŸå¤±
                    loss = self.criterion(outputs, targets, input_lengths, target_lengths)
                    total_loss += loss.item()
                    
                    # è§£ç é¢„æµ‹ç»“æœ
                    predicted_texts = self._decode_predictions(outputs)
                    
                    # è®¡ç®—å‡†ç¡®ç‡
                    for pred, gt in zip(predicted_texts, texts):
                        if pred == gt:
                            correct += 1
                        total += 1
                        
                except Exception as e:
                    print(f"éªŒè¯æ‰¹æ¬¡å‡ºé”™: {e}")
                    continue
        
        avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0
        accuracy = correct / total * 100 if total > 0 else 0
        
        self.logger.info(f'Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')
        
        return avg_loss, accuracy
    
    def _decode_predictions(self, outputs):
        """è§£ç é¢„æµ‹ç»“æœ"""
        predictions = []
        outputs = outputs.detach().cpu().numpy()
        
        # è½¬ç½®ç»´åº¦ï¼š(seq_len, batch, class) -> (batch, seq_len, class)
        outputs = outputs.transpose(1, 0, 2)
        
        for output in outputs:
            # ç®€å•çš„è´ªå¿ƒè§£ç 
            pred_chars = []
            prev_char = None
            
            for t in range(output.shape[0]):
                char_idx = np.argmax(output[t])
                
                # è·³è¿‡blankå’Œé‡å¤å­—ç¬¦
                if char_idx != len(self.charset) and char_idx != prev_char:
                    if char_idx < len(self.charset):
                        pred_chars.append(self.charset[char_idx])
                
                prev_char = char_idx
            
            predictions.append(''.join(pred_chars))
        
        return predictions


def create_sample_dataset():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®é›†"""
    # åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•
    sample_data = [
        {"image": "sample1.jpg", "text": "å¼ ä¸‰"},
        {"image": "sample2.jpg", "text": "æå››"},
        {"image": "sample3.jpg", "text": "1234567890"},
        {"image": "sample4.jpg", "text": "ç”·"},
        {"image": "sample5.jpg", "text": "æ±‰æ—"},
    ]
    
    os.makedirs('data/train', exist_ok=True)
    with open('data/train/labels.json', 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)
    
    # ä¹Ÿåˆ›å»ºtxtæ ¼å¼
    with open('data/train/labels.txt', 'w', encoding='utf-8') as f:
        for item in sample_data:
            f.write(f"{item['image']} {item['text']}\n")
    
    print("ç¤ºä¾‹æ•°æ®é›†å·²åˆ›å»º (JSONå’ŒTXTæ ¼å¼)")


def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒCRNNæ¨¡å‹')
    parser.add_argument('--data_dir', type=str, help='è®­ç»ƒæ•°æ®ç›®å½•')
    parser.add_argument('--label_file', type=str, help='æ ‡ç­¾æ–‡ä»¶')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹å¤§å°')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--save_dir', type=str, default='checkpoints', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--resume', type=str, help='æ¢å¤è®­ç»ƒçš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--create_sample', action='store_true', help='åˆ›å»ºç¤ºä¾‹æ•°æ®é›†')
    
    args = parser.parse_args()
    
    if args.create_sample:
        create_sample_dataset()
        return
    
    # é»˜è®¤ä½¿ç”¨ç”Ÿæˆçš„è®­ç»ƒæ•°æ®
    if not args.data_dir:
        args.data_dir = 'sample_training_data/crnn_data/images'
    if not args.label_file:
        args.label_file = 'sample_training_data/crnn_data/labels.txt'
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.data_dir):
        print(f"é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
        print("è¯·å…ˆè¿è¡Œ: python prepare_training_data.py --model crnn")
        return
    
    if not os.path.exists(args.label_file):
        print(f"é”™è¯¯: æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {args.label_file}")
        return
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å­—ç¬¦é›†
    charset = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' + \
              'ä¸­åäººæ°‘å…±å’Œå›½èº«ä»½è¯å§“åæ€§åˆ«æ°‘æ—å‡ºç”Ÿå¹´æœˆæ—¥ä½å€å…¬æ°‘å·ç ç­¾å‘æœºå…³æœ‰æ•ˆæœŸé™é•¿è‡³æ±‰æ—ç”·å¥³' + \
              'çœå¸‚å¿åŒºè¡—é“è·¯å·æ¥¼å®¤æ´¾å‡ºæ‰€å…¬å®‰å±€å…'
    
    print(f"å­—ç¬¦é›†å¤§å°: {len(charset)}")
    
    # åˆ›å»ºæ•°æ®é›†
    preprocessor = IDCardPreprocessor()
    
    train_dataset = TextDataset(
        args.data_dir, 
        args.label_file, 
        charset,
        transform=None  # æš‚æ—¶ä¸ä½¿ç”¨å˜æ¢
    )
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(train_dataset))
    val_size = len(train_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        train_dataset, [train_size, val_size]
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0  # è®¾ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0
    )
    
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset)}, éªŒè¯æ ·æœ¬: {len(val_dataset)}")
    
    # åˆ›å»ºæ¨¡å‹
    model = CRNN(
        img_h=32,
        nc=1,
        nclass=len(charset) + 1,  # +1 for blank
        nh=256
    ).to(device)
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=args.lr)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
    
    # è®­ç»ƒå™¨
    trainer = CTCTrainer(model, device, charset)
    
    # æ¢å¤è®­ç»ƒ
    start_epoch = 0
    if args.resume and os.path.exists(args.resume):
        checkpoint = torch.load(args.resume, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        print(f"ä»epoch {start_epoch}æ¢å¤è®­ç»ƒ")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    
    for epoch in range(start_epoch, args.epochs):
        print(f"\n=== Epoch {epoch+1}/{args.epochs} ===")
        
        # è®­ç»ƒ
        train_loss = trainer.train_epoch(train_loader, optimizer, epoch)
        
        # éªŒè¯
        val_loss, val_accuracy = trainer.validate(val_loader)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if val_loss < best_loss:
            best_loss = val_loss
            
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
                'val_accuracy': val_accuracy,
                'charset': charset
            }
            
            torch.save(checkpoint, os.path.join(args.save_dir, 'best_model.pth'))
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {best_loss:.4f}, å‡†ç¡®ç‡: {val_accuracy:.2f}%")
        
        # å®šæœŸä¿å­˜
        if epoch % 10 == 0:
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
                'val_accuracy': val_accuracy,
                'charset': charset
            }
            
            torch.save(checkpoint, os.path.join(args.save_dir, f'checkpoint_epoch_{epoch}.pth'))
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: epoch_{epoch}.pth")
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {args.save_dir}/best_model.pth")


if __name__ == "__main__":
    main() 